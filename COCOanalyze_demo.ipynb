{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## general imports\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "## COCO imports\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools.cocoanalyze import COCOanalyze\n",
    "\n",
    "## plotting imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## set paths\n",
    "dataDir  = '.'\n",
    "dataType = 'val2014'\n",
    "annType  = 'keypoints'\n",
    "teamName = 'fakekeypoints100'\n",
    "\n",
    "annFile  = '%s/annotations/%s_%s.json'%(dataDir, annType, dataType)\n",
    "resFile  = '%s/results/%s_%s_%s_results.json'%(dataDir, teamName, annType, dataType)\n",
    "\n",
    "print(\"{:10}[{}]\".format('annFile:',annFile))\n",
    "print(\"{:10}[{}]\".format('resFile:',annFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## initialize COCO ground-truth api\n",
    "coco_gt = COCO( annFile )\n",
    "imgIds  = sorted(coco_gt.getImgIds())\n",
    "imgIds  = imgIds[0:100]\n",
    "imgId   = imgIds[np.random.randint(100)]\n",
    "\n",
    "## initialize COCO detections api\n",
    "coco_dt   = coco_gt.loadRes( resFile )\n",
    "\n",
    "## initialize COCO analyze api\n",
    "coco_analyze = COCOanalyze(coco_gt, coco_dt, annType)\n",
    "coco_analyze.cocoEval.params.imgIds = imgIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use evaluate() method for standard coco evaluation\n",
    "# input arguments:\n",
    "#  - verbose   : verbose outputs    (default: False)\n",
    "#  - makeplots : plots eval results (default: False)\n",
    "#  - savedir   : path to savedir    (default: None)\n",
    "#  - team_name : team name string   (default: None)\n",
    "\n",
    "coco_analyze.evaluate(verbose=True, makeplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NOTE: the values below are all default\n",
    "\n",
    "# set OKS threshold of the extended error analysis\n",
    "coco_analyze.params.oksThrs       = [.85]\n",
    "\n",
    "# set OKS threshold required to match a detection to a ground truth\n",
    "coco_analyze.params.oksLocThrs    = .1\n",
    "\n",
    "# set KS threshold limits defining jitter errors\n",
    "coco_analyze.params.jitterKsThrs = [.5,.85]\n",
    "\n",
    "# set the localization errors to analyze and in what order\n",
    "# note: different order will show different progressive improvement\n",
    "# to study impact of single error type, study in isolation\n",
    "coco_analyze.params.err_types = ['miss','swap','inversion','jitter']\n",
    "\n",
    "# area ranges for evaluation\n",
    "# 'all' range is union of medium and large\n",
    "coco_analyze.params.areaRng       = [[32 ** 2, 1e5 ** 2]]#,[[32 ** 2, 96 ** 2]],[[96 ** 2, 1e5 ** 2]]\n",
    "coco_analyze.params.areaRngLbl    = ['all']#,['medium'],['large']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use analyze() method for advanced error analysis \n",
    "# input arguments:\n",
    "#  - check_kpts   : analyze keypoint localization errors for detections with a match (default: True)\n",
    "#                 : errors types are ['jitter','inversion','swap','miss']\n",
    "#  - check_scores : analyze optimal score (maximizing oks over all matches) for every detection (default: True)\n",
    "#  - check_bkgd   : analyze background false positives and false negatives (default: True)\n",
    "\n",
    "coco_analyze.analyze(check_kpts=True, check_scores=True, check_bkgd=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## after analyze() has been called the following variables are available\n",
    "\n",
    "# list of the corrected detections\n",
    "corrected_dts = coco_analyze.corrected_dts\n",
    "\n",
    "i = 17\n",
    "# info on keypoint detection localization error\n",
    "print 'good: %s'%corrected_dts[i]['good']\n",
    "print 'miss: %s'%corrected_dts[i]['miss']\n",
    "print 'swap: %s'%corrected_dts[i]['swap']\n",
    "print 'inv.: %s'%corrected_dts[i]['inversion']\n",
    "print 'jit.: %s\\n'%corrected_dts[i]['jitter']\n",
    "\n",
    "# corrected keypoint locations\n",
    "print 'predicted keypoints:\\n %s'%corrected_dts[i]['keypoints']\n",
    "print 'corrected keypoints:\\n %s\\n'%corrected_dts[i]['opt_keypoints']\n",
    "\n",
    "# optimal detection score\n",
    "print 'original score: %s'%corrected_dts[i]['score']\n",
    "print 'optimal score:  %s\\n'%corrected_dts[i]['opt_score']\n",
    "\n",
    "# dictionary with all detection and ground truth matches\n",
    "dt_gt_matches = coco_analyze.matches\n",
    "print \"Dt-gt match:\"\n",
    "for k in dt_gt_matches['dts'][corrected_dts[i]['id']][0]:\n",
    "    print k, dt_gt_matches['dts'][corrected_dts[i]['id']][0][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use summarize() method to get the results after progressive correction of errors\n",
    "# input arguments:\n",
    "#  - makeplots : plots eval results (default: False)\n",
    "#  - savedir   : path to savedir    (default: None)\n",
    "#  - team_name : team name string   (default: None)\n",
    "\n",
    "coco_analyze.summarize(makeplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## after summarize() has been called the following variables are available\n",
    "\n",
    "# list of the missed ground-truth annotations\n",
    "false_neg_gts = coco_analyze.false_neg_gts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## list the performance summary \n",
    "for stat in coco_analyze.stats: print stat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
